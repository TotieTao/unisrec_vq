import torch
import torch.nn as nn
import torch.distributed as dist
from util.tensor_op import concat_all_gather, GeLU
import numpy as np


def ema_inplace(moving_avg, new, decay):
    moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))


def ema_tensor_inplace(moving_avg, new, decay):
    new_out = torch.mul(new, 1.0 - decay)
    moving_avg.data.mul_(decay).add_(new_out.detach())


# point-to-point updates the group where user participate
def ema_tensor_mask_dict_inplace(moving_avg, new, mask, decay):
    new_out = torch.mul(new, 1 - decay)
    moving_avg[mask==1] = moving_avg.data[mask==1] * decay
    moving_avg.add_(new_out.detach())


# Each group has its own decay_rate
def ema_tensor_mask_group_inplace(moving_avg, new, mask, decay, cluster_update_num):
    for idx, m in enumerate(mask):
        if m == 0:
            continue
        if cluster_update_num[idx] <= 10:
            decay = decay * cluster_update_num[idx] / 10.
        new_out = new[idx] * (1-decay)
        moving_avg[idx] = moving_avg.data[idx] * decay
        moving_avg[idx] = moving_avg.data[idx] + new_out.detach()


def sum_inplace(sum_data, new):
    sum_data.data.add_(new)


def laplace_smoothing(x, n_categories, eps=1e-5):
    return (x + eps) / (x.sum() + n_categories * eps)


# def laplace_smoothing_dim(x, n_categories,dim=1, eps=1e-5):
#     return (x + eps) / (x.sum(dim=dim, keepdim=True) + n_categories * eps)

import math
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class InterestDict(nn.Module):
    def __init__(self, num_interest, dim_interest, decay=0.05, max_decay=0.99, eps=1e-5, topK=1):
        super(InterestDict, self).__init__()
        self._num_interest = num_interest
        self._dim_interest = dim_interest

        dictionary = torch.randn(num_interest, dim_interest)
        self.register_buffer('dictionary',dictionary)
        # nn.init.normal_(self.dictionary)
        self.register_buffer('cluster_size', torch.zeros(num_interest))
        self.register_buffer('cluster_size_sum', torch.zeros(num_interest))
        # self.register_buffer('cluster_value', torch.zeros(num_interest, dim_interest))
        self.register_buffer('num_update', torch.tensor(1))
        self.register_buffer('cluster_update_num', torch.ones(num_interest)) # group updated counts

        self.decay = decay
        self.eps = eps
        self.curr_decay = self.decay
        self.max_decay = max_decay
        self.K = topK
        self.init_dict_num = 0
        # projection_size = 384
        # self.proj_head1 = nn.Sequential(
        #     nn.Linear(dim_interest * topK, projection_size, bias=False),
        #     nn.LayerNorm(projection_size, eps=1e-12),
        #     # nn.ReLU(inplace=True),
        #     GeLU(),
        #     nn.Linear(projection_size, projection_size, bias=False),
        #     nn.LayerNorm(projection_size, eps=1e-12)
        # )
        # self.proj_head1.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def set_decay_updates(self, num_update):
        self.curr_decay = min(self.decay*num_update, self.max_decay)
        if num_update < 1000:
            self.num_update += 1

    def _init_dict_by_StdMean(self, input_flatten):
        # x_std,x_mean = torch.std_mean(torch.mean(input_flatten,dim=0))
        # nn.init.normal_(self.dictionary.detach(),std=float(x_std),mean=float(x_mean))

        x_min, x_max = torch.min(input_flatten), torch.max(input_flatten)
        nn.init.uniform_(self.dictionary.detach(), a=-1, b=1)
        # self.cluster_value.data.copy_(self.dictionary.clone())

    def _init_dict_by_user(self, input_flatten, idx_start):
        input_flatten_all = concat_all_gather(input_flatten.clone())
        input_len = input_flatten_all.shape[0]
        init_embed = [
            torch.mean(
                input_flatten_all[list(np.random.randint(0, input_len, size=2)), :],
                dim=0)
            for i in range(self._num_interest)
        ]
        init_embed = torch.stack(init_embed, dim=0)

        # self.init_dict_num += input_len
        #
        # if self._num_interest / self.init_dict_num > 1:
        #     self.dictionary.data[idx_start:idx_start + input_len, :] = input_flatten.clone()
        # else:
        #     if idx_start == 0:
        #         self.dictionary.data[idx_start:, :] = input_flatten[:self._num_interest, :].clone()
        #     else:
        #         self.dictionary.data[idx_start:, :] = input_flatten[:self._num_interest % idx_start, :].clone()

        torch.distributed.broadcast(init_embed, src=0)
        self.dictionary.data = init_embed
        # self.cluster_value.data.copy_(self.dictionary.clone())

    def _get_topK_emb(self, distances, K):
        """aggregate topK cluster embdeding to replace original user embedding"""
        distance, idx = torch.sort(distances, descending=False, dim=-1)
        distance = distance[:, :K]
        topk_idx = idx[:, :K]
        topk_emb = self.dictionary.data[topk_idx]  # batchsize, topk_cluster, emb_dim:b*c*d

        # if self.training:
        #     scale_min = torch.min(distance)
        #     scale_max = torch.max(distance)
        #     distance = (distance - scale_min) / (scale_max - scale_min)
        #     co_sim = torch.softmax(distance * -2, dim=1)
        #     group_emb = (co_sim.unsqueeze(dim=1) @ topk_emb).reshape([distances.shape[0], -1])
        # else:
        topk_emb_sum = torch.sum(topk_emb, dim=1)  # b*1*d
        group_emb = topk_emb_sum / K

        return group_emb, topk_idx

    def forward(self, inputs_flatten):
        # random initial
        if self.num_update == 1:
            # self._init_dict_by_StdMean(inputs_flatten)
            self._init_dict_by_user(inputs_flatten, 0)
            torch.distributed.barrier()

        print(inputs_flatten)
        print(self.dictionary)

        distances = (torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)
                     + torch.sum(self.dictionary.detach() ** 2, dim=1)
                     - 2 * torch.matmul(inputs_flatten, self.dictionary.data.t()))
        # distances = nn.CosineSimilarity(dim=2)(inputs_flatten.unsqueeze(1), self.dictionary.unsqueeze(0))
        group_emb, topk_idx = self._get_topK_emb(distances, self.K)
        # group_emb = self.proj_head1(group_emb)
        if self.training==False:
            return group_emb,topk_idx
        """
        encoding_indices: Tensor containing the discrete encoding indices, ie
        which element of the quantized space each input element was mapped to.
        """
        # encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encoding_indices = topk_idx
        print("encoding_indices:{}".format(encoding_indices))
        encodings = torch.zeros(encoding_indices.shape[0], self._num_interest, dtype=torch.float,
                                device=inputs_flatten.device)
        encodings.scatter_(1, encoding_indices, 1)  # one-hot
        # print(encodings)

        if self.training:
            self.set_decay_updates(self.num_update)

            # The following code is used for updating 'dictionary' buffer
            tmp_sum = torch.sum(encodings, dim=0, keepdim=True)
            self.cluster_size = torch.sum(concat_all_gather(tmp_sum), dim=0) # 1*cluster_num

            cluster_mask = torch.zeros_like(self.cluster_size)
            cluster_mask[self.cluster_size!=0] = 1

            sum_inplace(self.cluster_size_sum, self.cluster_size)
            sum_inplace(self.cluster_update_num, cluster_mask)

            input_sum_tmp = torch.matmul(encodings.t(), inputs_flatten)
            input_sum = torch.sum(concat_all_gather(input_sum_tmp.unsqueeze(dim=0)), dim=0) # cluster_num * dim

            world_size = dist.get_world_size()
            input_sum[cluster_mask==1] = input_sum[cluster_mask==1] / self.cluster_size[cluster_mask==1].unsqueeze(1)
            # ema_tensor_mask_dict_inplace(self.cluster_value, input_sum, cluster_mask, self.curr_decay)
            ema_tensor_mask_group_inplace(self.dictionary, input_sum, cluster_mask, self.max_decay,
                                          self.cluster_update_num)

            # print(self.cluster_value)
            # dist.all_reduce(self.cluster_value.div_(world_size))
            # print(self.cluster_value)
            # self.dictionary.data.copy_(self.cluster_value)

        print("update dict")
        print(self.cluster_size, self.cluster_size_sum, self.cluster_update_num)

        group_emb = (group_emb - inputs_flatten).detach() + inputs_flatten
        return group_emb, topk_idx


class InterestDictSoft(InterestDict):
    def __init__(self, num_interest, dim_interest, decay=0.05, max_decay=0.99, eps=1e-5, topK=1):
        super(InterestDictSoft, self).__init__(num_interest, dim_interest, decay, max_decay, eps, topK)

    def forward(self, inputs_flatten):
        # random initial
        if self.num_update == 1:
            # self._init_dict_by_StdMean(inputs_flatten)
            self._init_dict_by_user(inputs_flatten, 0)
            torch.distributed.barrier()
        print("inputs_flatten,dict")
        print(inputs_flatten)
        print(self.dictionary)
        distances = (torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)
                     + torch.sum(self.dictionary.detach() ** 2, dim=1)
                     - 2 * torch.matmul(inputs_flatten, self.dictionary.data.t()))
        # distances = nn.CosineSimilarity(dim=2)(inputs_flatten.unsqueeze(1), self.dictionary.unsqueeze(0))
        print("distence")
        print(distances)
        group_emb, topk_idx = self._get_topK_emb(distances,self.K)
        # top1_emb, _ = self._get_topK_emb(distances, 5)
        # top3_emb, _ = self._get_topK_emb(distances, 3)
        # top10_emb, _ = self._get_topK_emb(distances, 10)
        if self.training==False:
            return group_emb,topk_idx

        if self.training:
            self.set_decay_updates(self.num_update)

            # distances = nn.functional.normalize(distances, p=2, dim=1)
            scale_min = torch.min(distances)
            scale_max = torch.max(distances)
            distances = (distances - scale_min) / (scale_max - scale_min)


            encodings = torch.softmax(distances * -20, dim=1)
            print(encodings)
            print(torch.max(encodings, dim=0))
            # print(torch.nonzero(encodings))

            tmp_sum = torch.sum(encodings, dim=0, keepdim=True)
            self.cluster_size = torch.sum(concat_all_gather(tmp_sum), dim=0)  # 1*cluster_num

            cluster_mask = torch.zeros_like(self.cluster_size)
            cluster_mask[self.cluster_size != 0] = 1

            sum_inplace(self.cluster_size_sum, self.cluster_size)
            sum_inplace(self.cluster_update_num, cluster_mask)

            input_sum_tmp = torch.matmul(encodings.t(), inputs_flatten)
            input_sum = torch.sum(concat_all_gather(input_sum_tmp.unsqueeze(dim=0)), dim=0)  # cluster_num * dim
            # print(input_sum)
            input_sum[cluster_mask == 1] = input_sum[cluster_mask == 1] / self.cluster_size[
                cluster_mask == 1].unsqueeze(1)
            ema_tensor_mask_dict_inplace(self.dictionary, input_sum, cluster_mask, self.curr_decay)
            # ema_tensor_mask_group_inplace(self.dictionary, input_sum, cluster_mask, self.max_decay,
            #                               self.cluster_update_num)

            print("update dict")
            print(self.cluster_size, self.cluster_size_sum,self.cluster_update_num)

        group_emb = (group_emb - inputs_flatten).detach() + inputs_flatten
        # top1_emb = (top1_emb - inputs_flatten).detach() + inputs_flatten
        # top3_emb = (top3_emb - inputs_flatten).detach() + inputs_flatten
        # top10_emb = (top10_emb - inputs_flatten).detach() + inputs_flatten


        return group_emb, topk_idx

    def _get_dictionary(self):
        return self.dictionary.data
